# coding: utf-8

"""
    Together APIs

    The Together REST API. Please see https://docs.together.ai for more details.

    The version of the OpenAPI document: 2.0.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictFloat, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from together.generated.models.fine_tunes_post_request_train_on_inputs import (
    FineTunesPostRequestTrainOnInputs,
)
from together.generated.models.fine_tunes_post_request_training_type import (
    FineTunesPostRequestTrainingType,
)
from typing import Optional, Set
from typing_extensions import Self


class FineTunesPostRequest(BaseModel):
    """
    FineTunesPostRequest
    """  # noqa: E501

    training_file: StrictStr = Field(
        description="File-ID of a training file uploaded to the Together API"
    )
    validation_file: Optional[StrictStr] = Field(
        default=None,
        description="File-ID of a validation file uploaded to the Together API",
    )
    model: StrictStr = Field(
        description="Name of the base model to run fine-tune job on"
    )
    n_epochs: Optional[StrictInt] = Field(
        default=1, description="Number of epochs for fine-tuning"
    )
    n_checkpoints: Optional[StrictInt] = Field(
        default=1, description="Number of checkpoints to save during fine-tuning"
    )
    n_evals: Optional[StrictInt] = Field(
        default=0,
        description="Number of evaluations to be run on a given validation set during training",
    )
    batch_size: Optional[StrictInt] = Field(
        default=32, description="Batch size for fine-tuning"
    )
    learning_rate: Optional[Union[StrictFloat, StrictInt]] = Field(
        default=0.000010, description="Learning rate multiplier to use for training"
    )
    lr_scheduler: Optional[Dict[str, Any]] = None
    warmup_ratio: Optional[Union[StrictFloat, StrictInt]] = Field(
        default=0.0,
        description="The percent of steps at the start of training to linearly increase the learning rate.",
    )
    max_grad_norm: Optional[Union[StrictFloat, StrictInt]] = Field(
        default=1.0,
        description="Max gradient norm to be used for gradient clipping. Set to 0 to disable.",
    )
    weight_decay: Optional[Union[StrictFloat, StrictInt]] = Field(
        default=0.0, description="Weight decay"
    )
    suffix: Optional[StrictStr] = Field(
        default=None,
        description="Suffix that will be added to your fine-tuned model name",
    )
    wandb_api_key: Optional[StrictStr] = Field(
        default=None, description="API key for Weights & Biases integration"
    )
    wandb_base_url: Optional[StrictStr] = Field(
        default=None,
        description="The base URL of a dedicated Weights & Biases instance.",
    )
    wandb_project_name: Optional[StrictStr] = Field(
        default=None,
        description="The Weights & Biases project for your run. If not specified, will use `together` as the project name.",
    )
    wandb_name: Optional[StrictStr] = Field(
        default=None, description="The Weights & Biases name for your run."
    )
    train_on_inputs: Optional[FineTunesPostRequestTrainOnInputs] = False
    training_type: Optional[FineTunesPostRequestTrainingType] = None
    __properties: ClassVar[List[str]] = [
        "training_file",
        "validation_file",
        "model",
        "n_epochs",
        "n_checkpoints",
        "n_evals",
        "batch_size",
        "learning_rate",
        "lr_scheduler",
        "warmup_ratio",
        "max_grad_norm",
        "weight_decay",
        "suffix",
        "wandb_api_key",
        "wandb_base_url",
        "wandb_project_name",
        "wandb_name",
        "train_on_inputs",
        "training_type",
    ]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of FineTunesPostRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of lr_scheduler
        if self.lr_scheduler:
            _dict["lr_scheduler"] = self.lr_scheduler.to_dict()
        # override the default output from pydantic by calling `to_dict()` of train_on_inputs
        if self.train_on_inputs:
            _dict["train_on_inputs"] = self.train_on_inputs.to_dict()
        # override the default output from pydantic by calling `to_dict()` of training_type
        if self.training_type:
            _dict["training_type"] = self.training_type.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of FineTunesPostRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "training_file": obj.get("training_file"),
                "validation_file": obj.get("validation_file"),
                "model": obj.get("model"),
                "n_epochs": (
                    obj.get("n_epochs") if obj.get("n_epochs") is not None else 1
                ),
                "n_checkpoints": (
                    obj.get("n_checkpoints")
                    if obj.get("n_checkpoints") is not None
                    else 1
                ),
                "n_evals": obj.get("n_evals") if obj.get("n_evals") is not None else 0,
                "batch_size": (
                    obj.get("batch_size") if obj.get("batch_size") is not None else 32
                ),
                "learning_rate": (
                    obj.get("learning_rate")
                    if obj.get("learning_rate") is not None
                    else 0.000010
                ),
                "lr_scheduler": (
                    LRScheduler.from_dict(obj["lr_scheduler"])
                    if obj.get("lr_scheduler") is not None
                    else None
                ),
                "warmup_ratio": (
                    obj.get("warmup_ratio")
                    if obj.get("warmup_ratio") is not None
                    else 0.0
                ),
                "max_grad_norm": (
                    obj.get("max_grad_norm")
                    if obj.get("max_grad_norm") is not None
                    else 1.0
                ),
                "weight_decay": (
                    obj.get("weight_decay")
                    if obj.get("weight_decay") is not None
                    else 0.0
                ),
                "suffix": obj.get("suffix"),
                "wandb_api_key": obj.get("wandb_api_key"),
                "wandb_base_url": obj.get("wandb_base_url"),
                "wandb_project_name": obj.get("wandb_project_name"),
                "wandb_name": obj.get("wandb_name"),
                "train_on_inputs": (
                    FineTunesPostRequestTrainOnInputs.from_dict(obj["train_on_inputs"])
                    if obj.get("train_on_inputs") is not None
                    else None
                ),
                "training_type": (
                    FineTunesPostRequestTrainingType.from_dict(obj["training_type"])
                    if obj.get("training_type") is not None
                    else None
                ),
            }
        )
        return _obj
